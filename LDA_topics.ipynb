{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Created on Wed May  1 22:27:36 2019\n",
    "\n",
    "@author: Bhumika_Patoliya\n",
    "\"\"\"\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "\n",
    "path = \"C:\\\\Users\\\\patol\\\\Desktop\\\\MS CS\\\\sem 3\\\\BIA 660\\\\Project\\\\revised_midterm_df.csv\"\n",
    "orig_df = pd.read_csv(path)\n",
    "orig_df['raw_text'] = orig_df['review_text'].apply(lambda x: [y.strip().lower() for y in x.lower().replace(\"dr.\",\"doctor \").replace(\"\\r\",\". \").replace(\"\\n\",'. ').split(\". \") if len(y.strip()) > 1][1:])\n",
    "\n",
    "orig_grp = orig_df.groupby(['biz_name','link'])['raw_text'].sum().reset_index()\n",
    "temp = orig_grp[['raw_text']].sum()['raw_text']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "# Tokenize and lemmatize\n",
    "def preprocess(text):\n",
    "    result=[]\n",
    "    for token in gensim.utils.simple_preprocess(text) :\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "            \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original document: \n",
      "['This', 'disk', 'has', 'failed', 'many', 'times.', 'I', 'would', 'like', 'to', 'get', 'it', 'replaced.']\n",
      "\n",
      "\n",
      "Tokenized and lemmatized document: \n",
      "['disk', 'fail', 'time', 'like', 'replac']\n"
     ]
    }
   ],
   "source": [
    "document_num = 50\n",
    "doc_sample = 'This disk has failed many times. I would like to get it replaced.'\n",
    "\n",
    "print(\"Original document: \")\n",
    "words = []\n",
    "for word in doc_sample.split(' '):\n",
    "    words.append(word)\n",
    "print(words)\n",
    "print(\"\\n\\nTokenized and lemmatized document: \")\n",
    "print(preprocess(doc_sample))\n",
    "\n",
    "\n",
    "\n",
    "processed_docs = [preprocess(doc) for doc in temp]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = gensim.corpora.Dictionary(processed_docs)\n",
    "\n",
    "dictionary.filter_extremes(no_below=15, no_above=0.2, keep_n= 100000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_corpus = [dictionary.doc2bow(doc) for doc in processed_docs]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_model =  gensim.models.LdaMulticore(bow_corpus, \n",
    "                                   num_topics = 10, \n",
    "                                   id2word = dictionary,                                    \n",
    "                                   passes = 50,\n",
    "                                   workers = 4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic: 0 \n",
      "Words: 0.132*\"nurs\" + 0.074*\"help\" + 0.056*\"rude\" + 0.050*\"desk\" + 0.046*\"servic\" + 0.042*\"nice\" + 0.041*\"staff\" + 0.037*\"receptionist\" + 0.029*\"horribl\" + 0.027*\"girl\"\n",
      "\n",
      "\n",
      "Topic: 1 \n",
      "Words: 0.061*\"money\" + 0.042*\"differ\" + 0.038*\"away\" + 0.035*\"billing\" + 0.034*\"locat\" + 0.034*\"happi\" + 0.032*\"pay\" + 0.026*\"close\" + 0.025*\"night\" + 0.022*\"show\"\n",
      "\n",
      "\n",
      "Topic: 2 \n",
      "Words: 0.123*\"year\" + 0.079*\"kid\" + 0.071*\"pediatrician\" + 0.066*\"see\" + 0.046*\"children\" + 0.041*\"famili\" + 0.032*\"doctor\" + 0.029*\"go\" + 0.026*\"chang\" + 0.019*\"move\"\n",
      "\n",
      "\n",
      "Topic: 3 \n",
      "Words: 0.147*\"know\" + 0.045*\"send\" + 0.043*\"day\" + 0.030*\"hand\" + 0.028*\"wrong\" + 0.026*\"prescript\" + 0.026*\"later\" + 0.023*\"follow\" + 0.021*\"home\" + 0.020*\"say\"\n",
      "\n",
      "\n",
      "Topic: 4 \n",
      "Words: 0.087*\"want\" + 0.046*\"leav\" + 0.046*\"child\" + 0.043*\"hospit\" + 0.031*\"vaccin\" + 0.020*\"walk\" + 0.016*\"start\" + 0.015*\"school\" + 0.015*\"sign\" + 0.015*\"tell\"\n",
      "\n",
      "\n",
      "Topic: 5 \n",
      "Words: 0.196*\"time\" + 0.142*\"wait\" + 0.084*\"hour\" + 0.057*\"minut\" + 0.043*\"long\" + 0.037*\"appoint\" + 0.022*\"see\" + 0.021*\"take\" + 0.017*\"spend\" + 0.017*\"come\"\n",
      "\n",
      "\n",
      "Topic: 6 \n",
      "Words: 0.207*\"great\" + 0.141*\"experi\" + 0.093*\"think\" + 0.035*\"worst\" + 0.027*\"unprofession\" + 0.024*\"good\" + 0.023*\"overal\" + 0.021*\"unfortun\" + 0.018*\"posit\" + 0.016*\"servic\"\n",
      "\n",
      "\n",
      "Topic: 7 \n",
      "Words: 0.128*\"appoint\" + 0.057*\"feel\" + 0.046*\"schedul\" + 0.045*\"sick\" + 0.041*\"week\" + 0.041*\"get\" + 0.040*\"month\" + 0.039*\"better\" + 0.036*\"make\" + 0.031*\"comfort\"\n",
      "\n",
      "\n",
      "Topic: 8 \n",
      "Words: 0.083*\"say\" + 0.051*\"thank\" + 0.050*\"go\" + 0.035*\"tell\" + 0.031*\"issu\" + 0.024*\"treatment\" + 0.021*\"need\" + 0.019*\"surgeri\" + 0.017*\"infect\" + 0.015*\"help\"\n",
      "\n",
      "\n",
      "Topic: 9 \n",
      "Words: 0.096*\"visit\" + 0.080*\"insur\" + 0.048*\"medic\" + 0.036*\"problem\" + 0.027*\"bill\" + 0.024*\"offic\" + 0.024*\"charg\" + 0.020*\"pay\" + 0.019*\"record\" + 0.016*\"tell\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for idx, topic in lda_model.print_topics(-1):\n",
    "    print(\"Topic: {} \\nWords: {}\".format(idx, topic ))\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
